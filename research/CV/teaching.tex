%------------------------------------
% Dario Taraborelli
% Typesetting your academic CV in LaTeX
%
% URL: http://nitens.org/taraborelli/cvtex
% DISCLAIMER: This template is provided for free and without any guarantee 
% that it will correctly compile on your system if you have a non-standard  
% configuration.
% Some rights reserved: http://creativecommons.org/licenses/by-sa/3.0/
%------------------------------------

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[10pt, a4paper]{article}
\usepackage{fontspec} 
\usepackage[none]{hyphenat}
\usepackage{lastpage}


% DOCUMENT LAYOUT
\usepackage{geometry} 
\geometry{a4paper, textwidth=5.5in, textheight=10.0in, marginparsep=7pt, marginparwidth=.6in}
\setlength\parindent{0in}

% FONTS
\usepackage{xunicode}
\usepackage{xltxtra}
\defaultfontfeatures{Mapping=tex-text} % converts LaTeX specials (``quotes'' --- dashes etc.) to unicode
\setromanfont [Ligatures={Common}, BoldFont={Fontin Bold}, ItalicFont={Fontin Italic}]{Fontin}
\setsansfont [Ligatures={Common}, BoldFont={Fontin Sans Bold}, ItalicFont={Fontin Sans Italic}]{Fontin Sans}
\setmonofont[Scale=0.8]{Monaco} 
% ---- CUSTOM AMPERSAND
\newcommand{\amper}{{\fontspec[Scale=.95]{Fontin}\selectfont\itshape\&}}
% ---- MARGIN YEARS
\usepackage{marginnote}
\newcommand{\years}[1]{\marginnote{\scriptsize #1}}
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{7pt}
\reversemarginpar

% HEADINGS
\usepackage{sectsty} 
\usepackage[normalem]{ulem} 
\sectionfont{\rmfamily\mdseries\upshape\Large}
\subsectionfont{\rmfamily\bfseries\upshape\normalsize} 
\subsubsectionfont{\rmfamily\mdseries\upshape\normalsize} 

% PDF SETUP
% ---- FILL IN HERE THE DOC TITLE AND AUTHOR
\usepackage[xetex, bookmarks, colorlinks, breaklinks, pdftitle={Albert Einstein - vita},pdfauthor={My name}]{hyperref}  
\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=blue} 

% DOCUMENT
\begin{document}



\section*{Teaching Satement --- Sagar Malhotra}

\subsection*{Teaching Philosophy}
My teaching philosophy is rooted in the belief that students learn best by actively engaging with the material—\emph{learning by doing}.
I structure my courses around exercises, in-class problem-solving sessions, and collaborative activities that encourage students to apply concepts in real time.
Most of my lectures begin with a \emph{minimally non-trivial problem}—a problem that is easy to solve yet captures the essential ideas behind the topic at hand.
Students are then encouraged to collaborate with me and with one another to incrementally solve the problem while uncovering the underlying concepts.
This approach not only deepens conceptual understanding but also fosters critical thinking and builds confidence in tackling complex problems.

\subsection*{Teaching Experience}


\years{2025S - Now} \textbf{Modern Applications of Logic in ML\footnote{Extended to 6 ECTS VU as Neurosymbolic Reasoning for 2025W.}  
(VU, 3 ECTS, MSc. and Ph.D., TU Wien)\\ 
}
% (Elective graduate course in Computer Science, dean's approval awaited)\\ 
\textbf{Experience:}  Responsible for creating and teaching the entire course as a \textbf{solo instructor}. Created a new curriculum for graduate students interested in recent developments on the intersection of logic and machine learning. The course consists of four sections:
\begin{itemize}
    \item \textbf{Statistical relational learning}: models that integrate logic, probability and learning e.g., Markov logic, Problog etc. 
    \item \textbf{Algorithms}: ML relevant algorithmic problems in logic such as weighted model counting and MaxSAT 
    \item \textbf{Neurosymbolic AI:} models that integrate reasoning and learning in neural networks
    \item \textbf{Explainable AI:} Logic based explainability methods such as prime-implicant explanations.
    \item \textbf{Theoretical Foundations:} role of logic in learning theory and expressivity analysis of ML models like graph neural networks and transformers
\end{itemize}
I developed multiple exercises to familiarize students with state-of-the art research at the interface of logic and machine learning. As part of the course students also presented research papers, involving algorithms and formal proofs.
\textbf{The course was well attended, with all 15 of the 15 offered places taken-up by the students. The course evaluation also showed positive results.}\\


\years{2023W - Now} \textbf{Introduction to Machine Learning (VU, 6 ECTS, BS.c., TU Wien)}\\
\textbf{Experience:} I am part of the team that developed the first edition of this course in 2023. My key role was developing the module and exercises on Probabilistic Machine Learning. I also teach the lectures for Probabilistic Machine Learning and am responsible for the office hours for various modules of the course. I developed automatically graded python-based exercises that gave students hands-on experience. I also designed a large question bank for the theoretical exam. \\


\years{2023 - Now} \textbf{Machine Learning Algorithms and Applications 
(PR, 3 ECTS, M.Sc. and Ph.D., TU Wien)}\\
 \textbf{Experience:} This is a project based course organized by the Machine Learning Research Unit. I have consistently offered new projects in this course. One of the offered course led to an award-winning publication with a student, Alexander Pluska, at the \emph{International Conference on Principles of Knowledge Representation and Reasoning 2024}. \\ 
 
%  \newpage

 \years{2023W - Now} \textbf{Theoretical Foundations and Research Topics in Machine Learning\\
 (VU, 3 ECTS, M.Sc. and Ph.D., TU Wien)\\}
 \textbf{Experience:} This course introduces students to theoretical foundations of Machine Learning. Mainly, focusing on PAC learning, Kernel methods, Deep Learning and Graph Neural Networks.
 I conduct many of the exercises and course-work sessions on PAC learning and graph neural networks.\\ 

 \newpage 

 \years{2023W - Now} \textbf{Scientific Research and Writing (SE, 3 ECTS, B.Sc., TU Wien)}\\
\textbf{Experience:} This course is part of a compulsory scientific writing course in the Computer Science department of TU Wien. In the second part of the course, students are supervised in small groups by different research units with the goal of writing a seminar paper on state-of-the-art research. I supervise this course for the machine learning research unit. To this end, I set up an EasyChair based mock-conference. PhD students and PostDocs offer different topics for students. Students are then required to write a research paper under the supervision of an advisor and go through a peer-review experience acting as both authors and reviewers.   



 \section*{Student Supervision}
\subsection*{Master's students}
 \years{2025S-Now} \textbf{Thesis title}: Towards Enforcing Behaviors within
 Transformers using Differentiable Constraints \\
 \textbf{Student:} Michael Pritz,  TU Wien, Austria\\
 \textbf{Experience:} I am advising Michael for devising formal languages that specify constraints that can be compiled into differentiable loss terms that regularize transformers towards desirable behavior (fairness, robustness etc.). My main contribution as a supervisor has been guiding Michael towards a more formal and theoretically grounded approach of leveraging expressivity analysis of transformers for guiding transformer behavior. \\ 
 
 
 \years{2024W-2025S} \textbf{Thesis title}: Probabilistic Verification of Black-Box Systems \\
 \textbf{Student:} Peter Blohm,  TU Wien, Austria\\
 \textbf{Experience:} Our goal was to develop learning theory inspired quantitative verification approaches. 
 To this end, I assigned Peter initial readings on many learning theoretic concepts such as PAC-learning and $\epsilon$-nets. 
 We also interacted with another PhD student in the group and brought together a collaboration of nine leading researchers at TU Wien in the areas of machine learning and verification. 
 We wrote a joint paper (with Peter as a joint first author with the PhD student and myself as the last author) using the methods developed during the thesis research.
 The paper was accepted at the \emph{Next Generation of AI Safety} workshop at the \emph{International Conference on Machine Learning, ICML}. 
 Under my guidance, Peter then devised a novel, alternative approach to $\epsilon$-nets based verification, which was accepted for the main track of \emph{International Conference on Machine Learning, ICML}. 
%  Peter is currently investigating how to quantify the complexity of propositional formulas in classical and signal temporal logic to provide tight probabilistic bounds specific to the property that needs to be tested, instead of the conventional Chernoff bounds. 
 My key contribution in guiding Peter has been instilling rigorous analysis and clear scientific writing skills.\\ 

 \years{2022W-2023S}\textbf{Thesis Title}:  Lifted Inference beyond First Order Logic\\
 \textbf{Student Name}: Davide Bizzaro, FBK, Italy and University of Padova, Italy\\
 \textbf{Experience:} I co-supervised Davide with my PhD supervisor Luciano Serafini. Given Davide's mathematical background, I introduced him to the more theoretical and algorithmic aspects of Weighted Model Counting (WMC) in First Order Logic. He joined us when we had already developed ideas for extending WMC with acyclicity and connectivity axioms.  Given these developments, I had a hunch that forest axiom should also be obtainable. I initiated Davide in this direction with some initial pointers, and he was able to produce interesting results that became part of our \emph{Artificial Intelligence Journal} submission. I then indicated Davide towards the (much more challenging) problem of counting Markov Equivalence classes with arbitrary C$^2$ constraints. He developed a tractable novel method for computing the number of Markov Equivalence classes of size 1 on a DAG of size $n$. His work led to another preprint, currently being prepared for publication.
Davide finished his Masters with a 110 cum laude, the highest grade awardable to a Masters student.
 \vspace*{-1em}
 \subsection*{Other important Student Supervision Roles}
 
 \begin{itemize}
     \item Florian Chen, TU Wien, Austria\\
     Bachelor's Intern at the Machine Learning Research Unit, leading to a conference publication at ECML PKDD 2024
     \item Alexander Pluska,  TU Wien, Austria\\ Supervised in a graduate course, during which Alexander worked on a project under my supervision. Our work led to a conference publication at KR 2024
 \end{itemize}

\end{document}