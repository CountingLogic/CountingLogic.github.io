%------------------------------------
% Dario Taraborelli
% Typesetting your academic CV in LaTeX
%
% URL: http://nitens.org/taraborelli/cvtex
% DISCLAIMER: This template is provided for free and without any guarantee 
% that it will correctly compile on your system if you have a non-standard  
% configuration.
% Some rights reserved: http://creativecommons.org/licenses/by-sa/3.0/
%------------------------------------

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[10pt, a4paper]{article}
\usepackage{fontspec} 
\usepackage[none]{hyphenat}
\usepackage{lastpage}


% DOCUMENT LAYOUT
\usepackage{geometry} 
\geometry{a4paper, textwidth=5.5in, textheight=8.5in, marginparsep=7pt, marginparwidth=.6in}
\setlength\parindent{0in}

% FONTS
\usepackage{xunicode}
\usepackage{xltxtra}
\defaultfontfeatures{Mapping=tex-text} % converts LaTeX specials (``quotes'' --- dashes etc.) to unicode
\setromanfont [Ligatures={Common}, BoldFont={Fontin Bold}, ItalicFont={Fontin Italic}]{Fontin}
\setsansfont [Ligatures={Common}, BoldFont={Fontin Sans Bold}, ItalicFont={Fontin Sans Italic}]{Fontin Sans}
\setmonofont[Scale=0.8]{Monaco} 
% ---- CUSTOM AMPERSAND
\newcommand{\amper}{{\fontspec[Scale=.95]{Fontin}\selectfont\itshape\&}}
% ---- MARGIN YEARS
\usepackage{marginnote}
\newcommand{\years}[1]{\marginnote{\scriptsize #1}}
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{7pt}
\reversemarginpar

% HEADINGS
\usepackage{sectsty} 
\usepackage[normalem]{ulem} 
\sectionfont{\rmfamily\mdseries\upshape\Large}
\subsectionfont{\rmfamily\bfseries\upshape\normalsize} 
\subsubsectionfont{\rmfamily\mdseries\upshape\normalsize} 

% PDF SETUP
% ---- FILL IN HERE THE DOC TITLE AND AUTHOR
\usepackage[xetex, bookmarks, colorlinks, breaklinks, pdftitle={Albert Einstein - vita},pdfauthor={My name}]{hyperref}  
\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=blue} 

% DOCUMENT
\begin{document}


% Top conferences in machine learning and artificial intelligence have acceptance rates well below 30{\%}. Some conferences with their rank according to \href{CORE/ERA in brackets (for CORE/ERA A∗ indicates an international flagship conference, i.e., among the top 4, and A an internationally excellent conference, i.e., among the top 14 of all computer science conferences) at which I frequently publish are: International Conference on Machine Learning (A∗), AAAI Conference on Artificial Intelligence (A∗), Advances in Neural Information Processing Systems (A∗), Conference on Computational Learning Theory (A∗), ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (A∗), IEEE International Conference on Data Mining (A∗), Conference on Uncertainty in Artificial Intelligence (A∗), European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (A), and SIAM International Conference on Data Mining (A). The Machine Learning journal is one of the top journals in the field and is ranked as an international flagship according to CORE/ERA, i.e., it is among the top 7 of all computer science journals. Application oriented journal papers are typically published in respected journals in the field of the application.
\section*{Publications\protect\footnote{Supervised student coauthors are underlined.}}
% \vspace*{-1em}
% Supervised student collaborators are \underline{underlined}
% \\ \\

\years{2025}\textbf{Sagar Malhotra}, \underline{Davide Bizzaro} and Luciano Serafini\\
Lifted Inference beyond First Order Logic \\
\emph{Artificial Intelligence Journal}\\
\href{https://doi.org/10.1016/j.artint.2025.104310}{AIJ}\\ 

\years{2024}\underline{Alexander Pluska}, Pascal Welke, Thomas G{\"a}rtner and \textbf{Sagar Malhotra}.\\
Logical Distillation of Graph Neural Networks\\
\emph{International Conference on Principles of Knowledge Representation and Reasoning 2024} \\
\href{https://arxiv.org/abs/2406.07126}{KR 2024} (CORE Rank \textbf{A$^{*}$}, \textbf{17\%} acceptance rate in the special track)\\

\years{2024}\underline{Florian Chen}, Felix Weitkämper, and \textbf{Sagar Malhotra}.\\
Understanding Domain-Size Generalization in Markov Logic Networks\\
\emph{Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, ECML PKDD 2024} \\
\href{https://arxiv.org/abs/2403.15933}{ECML PKDD 2024} (CORE Rank \textbf{A}, \textbf{24\%} acceptance rate)\\

\years{2024}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini\\
Simple and Effective Transfer Learning for Neuro-Symbolic Integration\\
International Conference on Neural-Symbolic Learning and Reasoning, NeSy 2024\\
\href{https://arxiv.org/abs/2402.14047}{NeSy 2024} (\textbf{Accepted as full paper with a spotlight presentation})\\ 

\years{2023}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini. \\ Deep Symbolic Learning: Discovering Symbols and Rules from Perception \\ 
\emph{International Joint Conference on Artificial Intelligence 2023}\\
\href{https://www.ijcai.org/proceedings/2023/400}{IJCAI 2023} (CORE Rank \textbf{A$^{*}$}, \textbf{15\%} acceptance rate)\\ 


\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\
 On Projectivity in Markov Logic Networks \\ \emph{Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, ECML PKDD 2022} \\  
% \textbf{Largest European conference on machine learning with $\sim$1000 submissions and an \\ acceptance rate of $\sim$25\%} 
\href{https://link.springer.com/chapter/10.1007/978-3-031-26419-1_14}{ECML PKDD 2022}(CORE Rank \textbf{A}, \textbf{26\%} acceptance rate).\\ 


\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\ 
Weighted Model Counting in FO$^2$ with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula \\ \emph{AAAI Conference on Artificial Intelligence 2022}\\
% \textbf{Flagship AI conference  with $\sim$10000 submissions and an  acceptance rate of  $\sim$ 10\% for oral presentations}
\href{https://ojs.aaai.org/index.php/AAAI/article/view/20525}{AAAI 2022} (CORE Rank \textbf{A$^{*}$}, \textbf{15\%} acceptance rate, \textbf{accepted as oral presentation}) \\

\years{2021}\textbf{Sagar Malhotra} and Luciano Serafini\\
 A Combinatorial Approach to Weighted Model Counting in the Two Variable Fragment with Cardinality Constraints\\ \emph{International Conference of the Italian Association for Artificial Intelligence 2019}\\
\href{https://link.springer.com/chapter/10.1007/978-3-031-08421-8_10}{AIxIA 2021}

\newpage

\section*{Workshop Publications$^{1}$}
% \footnotemark[\value{footnote}]}
\noindent

\years{2024} Patrick Indri, \underline{Peter Blohm}, Anagha Athavale, Ezio Bartocci, Georg Weissenbacher, Matteo Maffei, Dejan Nickovic, Thomas Gärtner, \textbf{Sagar Malhotra}\\
Distillation based Robustness Verification with PAC Guarantees\\
\emph{Next Generation of AI Safety Workshop, ICML 2024}\\
\href{https://openreview.net/forum?id=vflefS3lmB}{NextGenAISafety, ICML 2024}\\

\years{2024}\underline{Alexander Pluska}, Pascal Welke, Thomas G{\"a}rtner and \textbf{Sagar Malhotra}.\\
Logical Distillation of Graph Neural Networks\\
\emph{Workshop on Mechanistic Interpratability, ICML 2024}\\
\href{https://openreview.net/forum?id=TfYnD2gYRO}{MI Workshop, ICML 2024}\\ \\

\years{2023}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini. \\ Deep Symbolic Learning: Discovering Symbols and Rules from Perception \\  
\emph{International Workshop on Neural-Symbolic Learning and Reasoning 2023}\\
\href{https://sites.google.com/view/nesy2023/home/nesy2023-programme-outline?authuser=0}{NeSy 2023} (\textbf{Accepted for spotlight presentation})\\ \\
\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\
On Projectivity in Markov Logic Networks\\ 
\emph{International Workshop on Probabilistic Logic Programming 2022}\\ \href{https://easychair.org/publications/preprint/2lTk}{ PLP 2022}\\ \\
% \newpage
% \emph{R.i.C.e.R.c.A: RCRA Incontri E Confronti, AIxIA 2022.} \href{https://ricerca2022.wordpress.com}{ R.i.C.e.R.c.A 2022}\\  \\
% \newpage
\years{2021}\textbf{Sagar Malhotra} and Luciano Serafini. Weighted Model Counting in FO$^2$ with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula\\ \emph{International Workshop on Statistical Relational AI, IJCLR 2021. }\\
\href{https://starai.cs.kuleuven.be/2021/}{ StarAI, IJCLR 2021} \\ \\ 
\years{{2020}}\textbf{Sagar Malhotra} and Luciano Serafini. Weighted Model Counting in C$^2$ (Abstract) \\
\emph{Workshop on Machine Learning and Data Mining, AIxIA 2020}\\
 \href{https://sites.google.com/view/mldm2020-workshop/program?authuser=0}{MLDM 2020}

%\newpage
% \section*{Under Review and Preprints}
% \years{2024}Floriann Chen,  Felix Weitkämper and \textbf{Sagar Malhotra}\\
% Understanding Domain-Size Generalization in Markov Logic Networks\\
% \emph{Under Review}\\
\section*{Preprints\footnote[1]{Supervised students coauthors are underlined}}
\years{2025}\underline{Peter Blohm}, Patrick Indri, Thomas Gärtner, \textbf{Sagar Malhotra}\\ 
Probably Approximately Global Robustness Certification\\
\href{https://countinglogic.github.io/files/Preprint.pdf}{Link} \\ \\  
\years{2025} Steve Azzolin$^{*}$, \textbf{Sagar Malhotra}$^{*}$, Andrea Passerini, Stefano Teso\\
Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective\\
$^*$Equal Contribution. \href{https://arxiv.org/abs/2502.02719}{Arxiv}\\ 

\years{2024} \underline{Davide Bizzaro}, Luciano Serafini and \textbf{Sagar Malhotra}\\
Towards Counting Markov Equivalence Classes with Logical Constraints\\
% \emph{Results from co-supervising Master's thesis of Davide Bizzaro}\\
\href{https://arxiv.org/abs/2405.13736}{Arxiv}\\ 




% \begin{center}
% \underline{DOCTORAL PROGRAM IN
% INFORMATION AND COMMUNICATION TECHNOLOGY}
%  \end{center}
% \textbf{Doctoral Candidate:} Sagar Malhotra \\ \\
% \textbf{Thesis Title:} On Tractability and Consistency of Probabilistic Inference in Relational Domains\\ \\
% \textbf{Advisor:} Luciano Serafini\\ \\
% \textbf{PhD Cycle:} 35$^{th}$
%\textbf{ PhD Cycle}
%  Fondazione Bruno Kessler\\
%  Via Sommarive, 18\\
% Trento, Trentino \texttt{38123}
% Italy\\
% Phone: \texttt{+39 320 841 2396}\\
% email: \href{mailto:smalhotra@fbk.eu}{smalhotra@fbk.eu}\\
% \textsc{url}: \href{https://countinglogic.github.io}{countinglogic.github.io}\\
% Born:  May 6, 1994\\
% Nationality:  Indian
% \section*{Personal Statement}
% I am a graduate researcher in Artificial Intelligence, working on probabilistic inference and learning in relational domains (e.g., graphs, databases, logic, etc.). My work has focused on developing efficient, expressive, and statistically consistent probabilistic relational models. Previously, I pursued a masters in physics with an exciting mix of quantitative biology and machine learning. I am passionate about solving and formulating rigorous foundational problems with a multi-disciplinary flavor. 
% I believe my background in logic, probability, physics, and machine learning can allow me to contribute significantly to research in foundational problems of emergence.

%%\hrul



\end{document}