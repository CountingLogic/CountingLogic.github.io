%------------------------------------
% Dario Taraborelli
% Typesetting your academic CV in LaTeX
%
% URL: http://nitens.org/taraborelli/cvtex
% DISCLAIMER: This template is provided for free and without any guarantee 
% that it will correctly compile on your system if you have a non-standard  
% configuration.
% Some rights reserved: http://creativecommons.org/licenses/by-sa/3.0/
%------------------------------------

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[10pt, a4paper]{article}
\usepackage{fontspec} 
\usepackage[none]{hyphenat}
\usepackage{lastpage}


% DOCUMENT LAYOUT
\usepackage{geometry} 
\geometry{a4paper, textwidth=5.5in, textheight=9.5in, marginparsep=7pt, marginparwidth=.6in}
\setlength\parindent{0in}

% FONTSs
\usepackage{xunicode}
\usepackage{xltxtra}
\defaultfontfeatures{Mapping=tex-text} % converts LaTeX specials (``quotes'' --- dashes etc.) to unicode
\setromanfont [Ligatures={Common}, BoldFont={Fontin Bold}, ItalicFont={Fontin Italic}]{Fontin}
\setsansfont [Ligatures={Common}, BoldFont={Fontin Sans Bold}, ItalicFont={Fontin Sans Italic}]{Fontin Sans}
\setmonofont[Scale=0.8]{Monaco} 
% ---- CUSTOM AMPERSAND
\newcommand{\amper}{{\fontspec[Scale=.95]{Fontin}\selectfont\itshape\&}}
% ---- MARGIN YEARS
\usepackage{marginnote}
\newcommand{\years}[1]{\marginnote{\scriptsize #1}}
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{7pt}
\reversemarginpar

% HEADINGS
\usepackage{sectsty} 
\usepackage[normalem]{ulem} 
\sectionfont{\rmfamily\mdseries\upshape\Large}
\subsectionfont{\rmfamily\bfseries\upshape\normalsize} 
\subsubsectionfont{\rmfamily\mdseries\upshape\normalsize} 

% PDF SETUP
% ---- FILL IN HERE THE DOC TITLE AND AUTHOR
\usepackage[xetex, bookmarks, colorlinks, breaklinks, pdftitle={Albert Einstein - vita},pdfauthor={My name}]{hyperref}  
\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=blue} 

% DOCUMENT
\begin{document}

\section*{%
Publications --- Dr. Sagar Malhotra\\[-.4em]%
}
% \vspace*{-2.0em}
% Supervised student collaborators are \underline{underlined}
% \\ \\
The following is a list of my peer-reviewed publications, ordered as journal publications, conference publications and (peer-reviewed) workshop contributions.
% Top conferences in artificial intelligence and machine learning typically have acceptance rates well below 30\%. 
I indicate the conference ranking using the \href{https://portal.core.edu.au/conf-ranks/}{CORE ranking system} in parentheses: \textbf{A$^{*}$} denotes a flagship international conference (top $\sim$8\% of all computer science conferences), while \textbf{A} signifies an internationally excellent conference (top $\sim$15\%).
The \emph{Artificial Intelligence Journal} is widely recognized as one of the leading journals in the fields of artificial intelligence and machine learning.\\


Since 2022, I have produced \textbf{8 top-tier publications} (i.e. in conferences ranked \textbf{A$^{*}$} or \textbf{A}, or in \textbf{Q1} journals) and \textbf{2 award winning publications}.
Additionally, I have supervised multiple students towards publishing their work. I have \uline{underlined} such supervised student coauthors. I have also indicated the relevant broader subfields (as listed in the call) for each publication where applicable.


% The following is a list of my peer-reviewed publications, including conference papers, journal articles, and workshop contributions.

% Top conferences in artificial intelligence and machine learning typically have acceptance rates well below 30\%. Conference rankings (according to the \href{https://portal.core.edu.au/conf-ranks/}{CORE ranking system}) are indicated in parentheses: \textbf{A$^{*}$} denotes a flagship international venue (top $\sim$8\% of all computer science conferences), and \textbf{A} an internationally excellent venue (top $\sim$15\%).

% The \emph{Artificial Intelligence Journal} is widely regarded as one of the top journals in artificial intelligence and machine learning.

% \uline{Supervised student coauthors are underlined}. Relevant publications are additionally marked by their corresponding broader subfields, as specified in the call.


% Top conferences in artificial intelligence and machine learning have acceptance rates well below
% 30\%. I have provided conference ranking according to \href{https://portal.core.edu.au/conf-ranks/}{CORE} in brackets (for CORE \textbf{A$^{*}$} indicates
% an international flagship conference, i.e., among the top $\sim$8\%, and \textbf{A} an internationally excellent conference,
% i.e., among the top $\sim$15\% of all computer science conferences). The \emph{Artificial Intelligence Journal} is widely regarded as one of the top journals in the field of artificial intelligence and machine learning.

% Student coauthors who were supervised are \uline{underlined}.
% Additionally, I highlight the broader subfields (as mentioned in the call) for relevant publications. 

\subsection*{Journal Publications}
\years{2025}\textbf{Sagar Malhotra}, \underline{Davide Bizzaro} and Luciano Serafini\\
Lifted Inference beyond First Order Logic \\
\emph{Artificial Intelligence Journal, 2025.}\\
\href{https://doi.org/10.1016/j.artint.2025.104310}{AIJ} (\textbf{Q1} Journal)
\vspace*{-0.5em}

\subsection*{Conference Publications}
\years{2025 \\ \textbf{Monitoring NNs}}\underline{Peter Blohm}, Patrick Indri, Thomas G채rtner, \textbf{Sagar Malhotra}\\ 
Probably Approximately Global Robustness Certification\\
\emph{International Conference of Machine Learning 2025.} \\
\href{https://openreview.net/forum?id=UKHlXpiFMy}{ICML 2025} (CORE Rank \textbf{A$^{*}$}, \textbf{26.9$\%$} acceptance rate) \\   

\years{2025 \\ \textbf{Explainable AI}} Steve Azzolin$^{\dagger}$, \textbf{Sagar Malhotra}$^{\dagger}$, Andrea Passerini, Stefano Teso \\
Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective\\
\emph{International Conference of Machine Learning 2025.}\\
\href{https://openreview.net/forum?id=mkqcUWBykZ}{ICML 2025} ($^{\dagger}$Equal Contribution, CORE Rank \textbf{A$^{*}$}, \textbf{26.9$\%$} acceptance rate)\\ 


% \newpage

\years{2024 \\ \textbf{Explainable AI}}\underline{Alexander Pluska}, Pascal Welke, Thomas G{\"a}rtner and \textbf{Sagar Malhotra}.\\
Logical Distillation of Graph Neural Networks\\
\emph{International Conference on Principles of Knowledge Representation and Reasoning 2024} \\
\href{https://arxiv.org/abs/2406.07126}{KR 2024} (CORE Rank \textbf{A$^{*}$}, \textbf{17\%} acceptance rate in the special track.  \textbf{Honorable Mention})\\

\years{2024 \\ \textbf{Supervised Learning}}\underline{Florian Chen}, Felix Weitk채mper, and \textbf{Sagar Malhotra}.\\
Understanding Domain-Size Generalization in Markov Logic Networks\\
\emph{Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference} \\
\href{https://arxiv.org/abs/2403.15933}{ECML PKDD 2024} (CORE Rank \textbf{A}, \textbf{24\%} acceptance rate)\\

\years{2024 \\ \textbf{Neuro- Symbolic AI}}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini\\
Simple and Effective Transfer Learning for Neuro-Symbolic Integration\\
\emph{International Conference on Neural-Symbolic Learning and Reasoning, NeSy 2024}\\
\href{https://arxiv.org/abs/2402.14047}{NeSy 2024} (\textbf{Best Paper Award})\\ 

\years{2023 \\ \textbf{Neuro- Symbolic AI}}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini. \\ Deep Symbolic Learning: Discovering Symbols and Rules from Perception \\ 
\emph{International Joint Conference on Artificial Intelligence 2023}\\
\href{https://www.ijcai.org/proceedings/2023/400}{IJCAI 2023} (CORE Rank \textbf{A$^{*}$}, \textbf{15\%} acceptance rate)\\ 

 \newpage

\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\
 On Projectivity in Markov Logic Networks \\ \emph{Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference} \\  
% \textbf{Largest European conference on machine learning with $\sim$1000 submissions and an \\ acceptance rate of $\sim$25\%} 
\href{https://link.springer.com/chapter/10.1007/978-3-031-26419-1_14}{ECML PKDD 2022} (CORE Rank \textbf{A}, \textbf{26\%} acceptance rate).\\ 



\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\ 
Weighted Model Counting in FO$^2$ with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula \\ \emph{AAAI Conference on Artificial Intelligence 2022}\\
% \textbf{Flagship AI conference  with $\sim$10000 submissions and an  acceptance rate of  $\sim$ 10\% for oral presentations}
\href{https://ojs.aaai.org/index.php/AAAI/article/view/20525}{AAAI 2022} (CORE Rank \textbf{A$^{*}$}, \textbf{15\%} acceptance rate, \textbf{accepted as oral presentation}) \\

\years{2021}\textbf{Sagar Malhotra} and Luciano Serafini\\
 A Combinatorial Approach to Weighted Model Counting in the Two Variable Fragment with Cardinality Constraints\\ \emph{International Conference of the Italian Association for Artificial Intelligence 2019}\\
\href{https://link.springer.com/chapter/10.1007/978-3-031-08421-8_10}{AIxIA 2021}

% \newpage
\subsection*{Workshop Publications (Peer-reviewed)}
% \footnotemark[\value{footnote}]}
\years{2025 \\ \textbf{Explainable AI}} \underline{Klaus Weinbauer}, Tieu-Long Phan, Peter F. Stadler,
Thomas G채rtner, and \textbf{Sagar Malhotra}\\
Prime Implicant Explanations for Reaction Feasibility Prediction\\
\emph{Workshop on Advances in Interpretable ML and AI, ECML-PKDD 2025}\\

\years{2024 \\ \textbf{Monitoring NNs}} Patrick Indri, \underline{Peter Blohm}, Anagha Athavale, Ezio Bartocci, Georg Weissenbacher, Matteo Maffei, Dejan Nickovic, Thomas G채rtner, \textbf{Sagar Malhotra}\\
Distillation based Robustness Verification with PAC Guarantees\\
\emph{Next Generation of AI Safety Workshop, ICML 2024}\\
\href{https://openreview.net/forum?id=vflefS3lmB}{NextGenAISafety, ICML 2024}\\

\years{2024 \\ \textbf{Explainable AI}}\underline{Alexander Pluska}, Pascal Welke, Thomas G{\"a}rtner and \textbf{Sagar Malhotra}.\\
Logical Distillation of Graph Neural Networks\\
\emph{Workshop on Mechanistic Interpretability, ICML 2024}\\
\href{https://openreview.net/forum?id=TfYnD2gYRO}{MI Workshop, ICML 2024}\\ \\
% \newpage
\years{2023 \\ \textbf{Neuro- Symbolic AI}}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini. \\ Deep Symbolic Learning: Discovering Symbols and Rules from Perception \\  
\emph{International Workshop on Neural-Symbolic Learning and Reasoning 2023}\\
\href{https://sites.google.com/view/nesy2023/home/nesy2023-programme-outline?authuser=0}{NeSy 2023} (\textbf{Accepted for spotlight presentation})\\ \\
\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\
On Projectivity in Markov Logic Networks\\ 
\emph{International Workshop on Probabilistic Logic Programming 2022}\\ \href{https://easychair.org/publications/preprint/2lTk}{ PLP 2022}\\ \\
% \newpage
% \emph{R.i.C.e.R.c.A: RCRA Incontri E Confronti, AIxIA 2022.} \href{https://ricerca2022.wordpress.com}{ R.i.C.e.R.c.A 2022}\\  \\
% \newpage
\years{2021}\textbf{Sagar Malhotra} and Luciano Serafini. Weighted Model Counting in FO$^2$ with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula\\ \emph{International Workshop on Statistical Relational AI, IJCLR 2021. }\\
\href{https://starai.cs.kuleuven.be/2021/}{ StarAI, IJCLR 2021}  
% \years{{2020}}\textbf{Sagar Malhotra} and Luciano Serafini. Weighted Model Counting in C$^2$ (Abstract) \\
% \emph{Workshop on Machine Learning and Data Mining, AIxIA 2020}\\
%  \href{https://sites.google.com/view/mldm2020-workshop/program?authuser=0}{MLDM 2020}\\
%\newpage
% \section*{Under Review and Preprints}
% \years{2024}Floriann Chen,  Felix Weitk채mper and \textbf{Sagar Malhotra}\\
% Understanding Domain-Size Generalization in Markov Logic Networks\\
% \emph{Under Review}\\
% \section*{Preprints\footnote[1]{Supervised students coauthors are underlined}}
% \years{2025}\underline{Peter Blohm}, Patrick Indri, Thomas G채rtner, \textbf{Sagar Malhotra}\\ 
% Probably Approximately Global Robustness Certification\\
% Under Review. 
% \href{https://countinglogic.github.io/files/Preprint.pdf}{Link} \\ \\  
% \years{2025} Steve Azzolin$^{\dagger}$, \textbf{Sagar Malhotra}$^{\dagger}$, Andrea Passerini, Stefano Teso\\
% Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective\\
% $^{\dagger}$Equal Contribution. Under Review. \href{https://arxiv.org/abs/2502.02719}{Arxiv}\\ 

% \years{2024} \underline{Davide Bizzaro}, Luciano Serafini and \textbf{Sagar Malhotra}\\
% Towards Counting Markov Equivalence Classes with Logical Constraints\\
% \emph{Results from co-supervising Master's thesis of Davide Bizzaro}\\
% \href{https://arxiv.org/abs/2405.13736}{Arxiv} 

% I can use the same footnote\footnotemark{} more than 
% once\footnotemark[\value{footnote}].
% \footnotetext{Supervised student collaborator.}

% \years{2023}\textbf{Sagar Malhotra}, Davide Bizzaro and Luciano Serafini\\
% Lifted Inference beyond First Order Logic\\
% \emph{Under Review at Artificial Intelligence Journal}\href{https://arxiv.org/abs/2302.09830}{\  arXiv:2302.09830}\\

% \years{2023}\textbf{Sagar Malhotra} and Luciano Serafini.\\
% Weighted First Order Model Counting with Directed Acyclic Graph Axiom\\
% \href{https://arxiv.org/abs/2302.09830}{arXiv:2302.09830}


% \section*{Ongoing Collaborations}
% \noindent

% Cooperative Artificial Intelligence \\
% Collaborators: Bruno Lepri (FBK, Italy), Luciano Serafini (FBK, Italy)\\ 

% Graphon Estimation and Inference for Relational Structures\\
% Collaborators: Manfred Jaeger (Aalborg University, Denmark), Luciano Serafini (FBK, Italy) 


% My research revolves around formal analysis of probability distributions over relational structures. I am especially interested in:

% $\cdot$ Random graph models and their extension to more complex relational domains\\
% $\cdot$ Exact and approximate probabilistic inference  \\
% $\cdot$ Combinatorics over relational structures\\ 
% $\cdot$ Consistency of probabilistic inference\\
% $\cdot$ Estimating asymptotic properties from sub-sampled relational structures\\
% \newpage
\end{document}