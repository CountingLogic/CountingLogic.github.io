%------------------------------------
% Dario Taraborelli
% Typesetting your academic CV in LaTeX
%
% URL: http://nitens.org/taraborelli/cvtex
% DISCLAIMER: This template is provided for free and without any guarantee 
% that it will correctly compile on your system if you have a non-standard  
% configuration.
% Some rights reserved: http://creativecommons.org/licenses/by-sa/3.0/
%------------------------------------

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[10pt, a4paper]{article}
\usepackage{fontspec} 
\usepackage[none]{hyphenat}
\usepackage{lastpage}


% DOCUMENT LAYOUT
\usepackage{geometry} 
\geometry{a4paper, textwidth=5.5in, textheight=9.5in, marginparsep=7pt, marginparwidth=.6in}
\setlength\parindent{0in}

% FONTSs
\usepackage{xunicode}
\usepackage{xltxtra}
\defaultfontfeatures{Mapping=tex-text} % converts LaTeX specials (``quotes'' --- dashes etc.) to unicode
\setromanfont [Ligatures={Common}, BoldFont={Fontin Bold}, ItalicFont={Fontin Italic}]{Fontin}
\setsansfont [Ligatures={Common}, BoldFont={Fontin Sans Bold}, ItalicFont={Fontin Sans Italic}]{Fontin Sans}
\setmonofont[Scale=0.8]{Monaco} 
% ---- CUSTOM AMPERSAND
\newcommand{\amper}{{\fontspec[Scale=.95]{Fontin}\selectfont\itshape\&}}
% ---- MARGIN YEARS
\usepackage{marginnote}
\newcommand{\years}[1]{\marginnote{\scriptsize #1}}
\renewcommand*{\raggedleftmarginnote}{}
\setlength{\marginparsep}{7pt}
\reversemarginpar

% HEADINGS
\usepackage{sectsty} 
\usepackage[normalem]{ulem} 
\sectionfont{\rmfamily\mdseries\upshape\Large}
\subsectionfont{\rmfamily\bfseries\upshape\normalsize} 
\subsubsectionfont{\rmfamily\mdseries\upshape\normalsize} 

% PDF SETUP
% ---- FILL IN HERE THE DOC TITLE AND AUTHOR
\usepackage[xetex, bookmarks, colorlinks, breaklinks, pdftitle={Albert Einstein - vita},pdfauthor={My name}]{hyperref}  
\hypersetup{linkcolor=blue,citecolor=blue,filecolor=black,urlcolor=blue} 

% DOCUMENT
\begin{document}

\section*{%
Publications --- Dr. Sagar Malhotra\\[-.4em]%
}
% \vspace*{-2.0em}
% Supervised student collaborators are \underline{underlined}
% \\ \\
The following is a list of my peer-reviewed publications, ordered as conference, journal and selected (peer-reviewed) workshop contributions.
% Top conferences in artificial intelligence and machine learning typically have acceptance rates well below 30\%. 
I indicate the conference ranking using the \href{https://portal.core.edu.au/conf-ranks/}{CORE ranking system} in parentheses: \textbf{A$^{*}$} denotes a flagship international conference (top $\sim$8\% of all computer science conferences), while \textbf{A} signifies an internationally excellent conference (top $\sim$15\%).
The \emph{Artificial Intelligence Journal} is widely recognized as one of the leading journals in the fields of artificial intelligence and machine learning.\\


Since 2022, I have produced \textbf{9 top-tier publications} (i.e. in conferences ranked \textbf{A$^{*}$} or \textbf{A}, or in \textbf{Q1} journals) and \textbf{2 award winning publications}.
Additionally, I have supervised multiple students, in different roles, leading to publications. I have \uline{underlined} such supervised student coauthors. 
% I have also indicated the relevant broader subfields (as listed in the call) for each publication where applicable.
\subsection*{Conference Publications}
\years{2025}\underline{Alexander Pluska} and \textbf{Sagar Malhotra}\\ 
On Local Limits of Sparse Random Graphs:\\ Color Convergence and the Refined Configuration Model\\
\emph{Neural Information Processing Systems 2025.} \\
\href{https://neurips.cc/virtual/2025/poster/116124}{NeurIPS 2025} (CORE Rank \textbf{A$^{*}$}, \textbf{24.52$\%$} acceptance rate) \\   

\years{2025}\underline{Peter Blohm}, Patrick Indri, Thomas Gärtner, \textbf{Sagar Malhotra}\\ 
Probably Approximately Global Robustness Certification\\
\emph{International Conference of Machine Learning 2025.} \\
\href{https://openreview.net/forum?id=UKHlXpiFMy}{ICML 2025} (CORE Rank \textbf{A$^{*}$}, \textbf{26.9$\%$} acceptance rate) \\   

\years{2025} Steve Azzolin$^{\dagger}$, \textbf{Sagar Malhotra}$^{\dagger}$, Andrea Passerini, Stefano Teso \\
Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective\\
\emph{International Conference of Machine Learning 2025.}\\
\href{https://openreview.net/forum?id=mkqcUWBykZ}{ICML 2025} ($^{\dagger}$Equal Contribution, CORE Rank \textbf{A$^{*}$}, \textbf{26.9$\%$} acceptance rate)\\ 


% \newpage

\years{2024}\underline{Alexander Pluska}, Pascal Welke, Thomas G{\"a}rtner and \textbf{Sagar Malhotra}.\\
Logical Distillation of Graph Neural Networks\\
\emph{International Conference on Principles of Knowledge Representation and Reasoning 2024} \\
\href{https://arxiv.org/abs/2406.07126}{KR 2024} (CORE Rank \textbf{A$^{*}$}, \textbf{17\%} acceptance rate in the special track.  \textbf{Honorable Mention})\\

\years{2024}\underline{Florian Chen}, Felix Weitkämper, and \textbf{Sagar Malhotra}.\\
Understanding Domain-Size Generalization in Markov Logic Networks\\
\emph{Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference} \\
\href{https://arxiv.org/abs/2403.15933}{ECML PKDD 2024} (CORE Rank \textbf{A}, \textbf{24\%} acceptance rate)\\

\years{2024}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini\\
Simple and Effective Transfer Learning for Neuro-Symbolic Integration\\
\emph{International Conference on Neural-Symbolic Learning and Reasoning, NeSy 2024}\\
\href{https://arxiv.org/abs/2402.14047}{NeSy 2024} (\textbf{Best Paper Award})\\ 

\years{2023}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini. \\ Deep Symbolic Learning: Discovering Symbols and Rules from Perception \\ 
\emph{International Joint Conference on Artificial Intelligence 2023}\\
\href{https://www.ijcai.org/proceedings/2023/400}{IJCAI 2023} (CORE Rank \textbf{A$^{*}$}, \textbf{15\%} acceptance rate)\\ 

\newpage

\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\
 On Projectivity in Markov Logic Networks \\ \emph{Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference} \\  
% \textbf{Largest European conference on machine learning with $\sim$1000 submissions and an \\ acceptance rate of $\sim$25\%} 
\href{https://link.springer.com/chapter/10.1007/978-3-031-26419-1_14}{ECML PKDD 2022} (CORE Rank \textbf{A}, \textbf{26\%} acceptance rate).\\ 

% \newpage

\years{2022}\textbf{Sagar Malhotra} and Luciano Serafini\\ 
Weighted Model Counting in FO$^2$ with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula \\ \emph{AAAI Conference on Artificial Intelligence 2022}\\
% \textbf{Flagship AI conference  with $\sim$10000 submissions and an  acceptance rate of  $\sim$ 10\% for oral presentations}
\href{https://ojs.aaai.org/index.php/AAAI/article/view/20525}{AAAI 2022} (CORE Rank \textbf{A$^{*}$}, \textbf{15\%} acceptance rate, \textbf{accepted as oral presentation}) \\

\years{2021}\textbf{Sagar Malhotra} and Luciano Serafini\\
 A Combinatorial Approach to Weighted Model Counting in the Two Variable Fragment with Cardinality Constraints\\ \emph{International Conference of the Italian Association for Artificial Intelligence 2019}\\
\href{https://link.springer.com/chapter/10.1007/978-3-031-08421-8_10}{AIxIA 2021}

\subsection*{Journal Publications}
\years{2025}\textbf{Sagar Malhotra}, \underline{Davide Bizzaro} and Luciano Serafini\\
Lifted Inference beyond First Order Logic \\
\emph{Artificial Intelligence Journal.}\\
\href{https://doi.org/10.1016/j.artint.2025.104310}{AIJ} (\textbf{Q1} Journal)
% \vspace*{-0.5em}

\subsection*{Under Review}
\years{2025} Steve Azzolin, Stefano Teso, Bruno Lepri, Andrea Passerini, and \textbf{Sagar Malhotra}\\
GNN Explanations that do not Explain and How to find Them\\
\emph{Under Review at International Conference on Learning Representations 2025.}\\
Pre-rebuttal Score: Accept (8), Weak Accept (6) and Weak Reject (4) \\
\href{https://openreview.net/forum?id=HBcgLe6NZD}{ICLR 2025}

\subsection*{Selected Workshop Publications (Peer-reviewed)}
% \footnotemark[\value{footnote}]}
\years{2025} \underline{Anton Zamyatin}, Patrick Indri, \textbf{Sagar Malhotra} and Thomas Gärtner\\
Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles\\
\emph{Epistemic Intelligence in  Machine Learning Workshop, EurIPS 2025}\\ \href{https://sites.google.com/view/eiml-eurips2025/home}{ EIML@EurIPS2025} \\

\years{2025} \underline{Klaus Weinbauer}, Tieu-Long Phan, Peter F. Stadler,
Thomas Gärtner, and \textbf{Sagar Malhotra}\\
Prime Implicant Explanations for Reaction Feasibility Prediction\\
\emph{Workshop on Advances in Interpretable ML and AI, ECML-PKDD 2025}\\
\href{https://arxiv.org/abs/2510.09226}{AIMLAI, ECML PKDD 2025}\\

\years{2024} Patrick Indri, \underline{Peter Blohm}, Anagha Athavale, Ezio Bartocci, Georg Weissenbacher, Matteo Maffei, Dejan Nickovic, Thomas Gärtner, \textbf{Sagar Malhotra}\\
Distillation based Robustness Verification with PAC Guarantees\\
\emph{Next Generation of AI Safety Workshop, ICML 2024}\\
\href{https://openreview.net/forum?id=vflefS3lmB}{NextGenAISafety, ICML 2024}\\

% \years{2024}\underline{Alexander Pluska}, Pascal Welke, Thomas G{\"a}rtner and \textbf{Sagar Malhotra}.\\
% Logical Distillation of Graph Neural Networks\\
% \emph{Workshop on Mechanistic Interpretability, ICML 2024}\\
% \href{https://openreview.net/forum?id=TfYnD2gYRO}{MI Workshop, ICML 2024}\\ \\
% \newpage
\years{2023}Alessandro Daniele, Tommaso Campari, \textbf{Sagar Malhotra} and Luciano Serafini. \\ Deep Symbolic Learning: Discovering Symbols and Rules from Perception \\  
\emph{International Workshop on Neural-Symbolic Learning and Reasoning 2023}\\
\href{https://sites.google.com/view/nesy2023/home/nesy2023-programme-outline?authuser=0}{NeSy 2023} (\textbf{Accepted for spotlight presentation})
\end{document}